v1: 原版，除了梯度剪裁，batch size 5000
v2: 原版加 梯度裁剪  batch size 5000
v3: 原版 加梯度剪裁 加改动的交叉熵 ds[:10000] batch size = 32
v4: 线性层改为256个神经元，加梯度裁剪，加改动的交叉熵，ds[:10000] batch size = 16，加收集热图
V5: softmax加到网络里面，评估函数那边的取消。只有一遍softmax
V6: 对交叉熵增加很小的数, drop out =0.3 linear layer = 512
V7: batch size = 16， learning rate = 1e-6
V8: learning rate = 1e-5
V9: 使用adam优化器，batch size = 24
V10: linear cell num = 529, dropout = 0.5
V11: 添加noise variance=0.01 linear cell num=512
V12: dataset全集， epoch=3000
V13: epoch=50 linear没有bias 加噪声 linear加正则 linear num=256 batch size=10